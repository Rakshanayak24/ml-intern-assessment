ğŸŒŸ Trigram Language Model + Scaled Dot-Product Attention
Desible AI/ML Internship Assessment â€” Completed by Raksha Nayak

This repository contains my end-to-end implementation of both tasks from the Desible AI / ML Internship Assessment:

âœ” Task 1 â€” Trigram (N=3) Language Model (from scratch)
âœ” Task 2 â€” Scaled Dot-Product Attention using NumPy only (optional, completed)

The project is modular, clean, unit-tested, and includes runnable demos.

ğŸ“¥ Run in Google Colab (Recommended)

You can run the full project inside Colab using:

!git clone https://github.com/Rakshanayak24/ml-intern-assessment.git
%cd ml-intern-assessment/ml-assignment
!pip install -r requirements.txt


Now you're ready to run the model or attention demo.

ğŸ—ï¸ Project Structure
ml-assignment/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ example_corpus.txt
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ngram_model.py        # Trigram model implementation
â”‚   â”œâ”€â”€ utils.py
â”‚   â””â”€â”€ generate.py           # Train + generate text
â”‚
â”œâ”€â”€ attention/
â”‚   â”œâ”€â”€ attention.py          # Scaled Dot-Product Attention (NumPy-only)
â”‚   â”œâ”€â”€ demo.py               # Demo script
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_ngram.py         # Unit tests
â”‚
â”œâ”€â”€ README.md                 # Documentation
â””â”€â”€ evaluation.md             # 1-page design choices summary

ğŸš€ Task 1 â€” Trigram Language Model
ğŸ“Œ Install Dependencies
pip install -r requirements.txt

ğŸ“Œ Train & Generate Text
python src/generate.py


This will:

Load & clean the corpus

Build trigram counts

Compute probabilities

Generate new text using probabilistic sampling

ğŸ§ª Run Unit Tests
pytest tests/test_ngram.py


All tests should pass with the final implementation.

ğŸ§  Task 2 â€” Scaled Dot-Product Attention (NumPy Only)

Implementation located in:

attention/attention.py


Run the demo:

python attention/demo.py

Example Output:
Q = [[1 0]]
K = [[1 1]]
V = [[0.5 2. ]]

Attention Weights:
[[1.]]

Attention Output:
[[0.5 2. ]]
