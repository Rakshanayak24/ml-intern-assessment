
  ğŸ§  AI/ML Assignment â€” Trigram Language Model + Scaled Dot-Product Attention

  This repository contains two core components demonstrating foundations of classical NLP and modern deep learning:

  **Task 1 â€” Trigram Language Model (N=3)**
  Implemented fully from scratch using Python, including text preprocessing, n-gram counting, probability computation, and sampling-based text generation.

  **Task 2 â€” Scaled Dot-Product Attention (Optional)**
  A NumPy-only implementation of the core operation behind Transformer architectures (BERT, GPT, etc.), including a demo script.

  This assignment showcases clean code design, probabilistic modeling, understanding of linear algebra, and modular project structure.

  ğŸ“‚ **Project Structure**

      ml-assignment/
      â”œâ”€â”€ data/
      â”‚   â””â”€â”€ example_corpus.txt
      â”œâ”€â”€ src/
      â”‚   â”œâ”€â”€ ngram_model.py
      â”‚   â”œâ”€â”€ utils.py
      â”‚   â””â”€â”€ generate.py
      â”œâ”€â”€ attention/
      â”‚   â”œâ”€â”€ attention.py
      â”‚   â””â”€â”€ demo_attention.py
      â”œâ”€â”€ tests/
      â”‚   â””â”€â”€ test_ngram.py
      â”œâ”€â”€ requirements.txt
      â”œâ”€â”€ evaluation.md
      â””â”€â”€ README.md

  ğŸš€ **How to Run the Project**

  **1ï¸âƒ£ Create and Activate Virtual Environment**
  ```bash
  python3 -m venv venv
  source venv/bin/activate      # Linux/Mac
  venv\Scripts\activate         # Windows
2ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

ğŸŸ¦ TASK 1 â€” TRIGRAM LANGUAGE MODEL

â–¶ï¸ Run the Trigram Generator

python -m src.generate

This will:
- Read corpus from data/example_corpus.txt
- Clean and tokenize text
- Train a trigram language model
- Print generated text using probabilistic sampling

ğŸ”§ How It Works (Short Explanation)

- Text is cleaned â†’ lowercased, punctuation removed
- `<s>` and `</s>` tokens mark sentence boundaries
- Trigrams `(w1, w2, w3)` are counted in a nested dictionary
- Probabilities computed as:
  `P(w3 | w1, w2) = count(w1, w2, w3) / sum(count(w1, w2, *))`
- Text generation starts with `<s>, <s>`, samples next words, and stops at `</s>` or max length

(Full explanation is included in evaluation.md.)

ğŸ§ª Run Pytests

pytest -v

Validates:
- Model training
- Text generation
- Empty text handling
- Short text behavior


behavior

ğŸŸ§ TASK 2 â€” SCALED DOT-PRODUCT ATTENTION (Optional)

Uses the transformer formula:

Attention(Q, K, V) = softmax( QKáµ€ / âˆšd_k ) Â· V

Where:
- Q â†’ Queries
- K â†’ Keys
- V â†’ Values
- dâ‚– â†’ Key dimensionality

â–¶ï¸ How to Run the Attention Demo

cd attention
python demo_attention.py

The script:
- Creates random Q, K, V matrices
- Calls scaled_dot_product_attention()
- Prints:
- Attention Output
- Attention Weights (Softmax matrix)

ğŸ§ª Manual Testing Example (Optional)

import numpy as np
from attention import scaled_dot_product_attention

Q = np.random.rand(1, 3, 4)
K = np.random.rand(1, 3, 4)
V = np.random.rand(1, 3, 4)

output, weights = scaled_dot_product_attention(Q, K, V)
print(output)
print(weights)


