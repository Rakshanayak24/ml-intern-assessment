
  ğŸ§  AI/ML Assignment â€” Trigram Language Model + Scaled Dot-Product Attention

  This repository contains two core components demonstrating foundations of classical NLP and modern deep learning:

  **Task 1 â€” Trigram Language Model (N=3)**
  Implemented fully from scratch using Python, including text preprocessing, n-gram counting, probability computation, and sampling-based text generation.

  **Task 2 â€” Scaled Dot-Product Attention (Optional)**
  A NumPy-only implementation of the core operation behind Transformer architectures (BERT, GPT, etc.), including a demo script.
# Trigram Language Model + Self-Attention Module  

    ---

    ## ğŸ“Œ Project Structure

    ```
    .
    â”œâ”€â”€ data/
    â”‚   â””â”€â”€ input.txt
    â”œâ”€â”€ trigram/
    â”‚   â”œâ”€â”€ model.py
    â”‚   â”œâ”€â”€ utils.py
    â”‚   â””â”€â”€ __init__.py
    â”œâ”€â”€ attention/
    â”‚   â”œâ”€â”€ attention.py
    â”‚   â””â”€â”€ __init__.py
    â”œâ”€â”€ README.md
    â”œâ”€â”€ evaluation.md
    â””â”€â”€ config.yml
    ```

    ---

    # ğŸš€ How to Run (Both Models)

    ## 1ï¸âƒ£ **Run Trigram Language Model**

    ### **Step 1 â€” Install requirements**
    ```bash
    pip install -r requirements.txt
    ```
    (Only uses standard Python libraries; no heavy dependencies.)

    ### **Step 2 â€” Train the model**
    ```bash
    python trigram/model.py --train data/input.txt --save model.pkl
    ```

    ### **Step 3 â€” Generate text**
    ```bash
    python trigram/model.py --generate model.pkl --seed "the world"
    ```

    Output sample:
    ```
    the world is full of amazing discoveries waiting to be explored ...
    ```

    ---

    ## 2ï¸âƒ£ **Run the Attention Module**

    ### **Step 1 â€” Simply import and run**
    ```bash
    python attention/attention.py
    ```

    ### **What it does**
    - Builds token embeddings  
    - Computes Queryâ€“Keyâ€“Value  
    - Applies scaled dot-product attention  
    - Returns attention-weighted representations  

    ### **Example output**
    ```
    Attention weights:
    [[0.21 0.54 0.25]
     [0.33 0.18 0.49]
     [0.40 0.12 0.48]]

    Context vectors:
    [[...token 1...]
     [...token 2...]
     [...token 3...]]
    ```

    ---

    # ğŸ§  Summary of What This Repo Demonstrates
    âœ” Understanding of classical NLP modeling (Trigrams)  
    âœ” Ability to implement sampling-based text generation  
    âœ” Working knowledge of attention (core foundation of Transformers)  
    âœ” Clean code and reproducible execution  
    âœ” Real-world ML workflow (training â†’ saving â†’ loading â†’ inference)  

    ---

    # ğŸ“„ Evaluation  
    Please see **evaluation.md** for the required 1-page design summary.

  
    author: "Raksha Nayak"
    purpose: "Submission-ready ML/NLP project for internship selection"
    last_updated: "2025-11-21"


