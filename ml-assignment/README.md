ğŸ§  AI/ML Assignment â€” Trigram Language Model + Scaled Dot-Product Attention

This repository contains two core components demonstrating foundations of classical NLP and modern deep learning:

Task 1 â€” Trigram Language Model (N=3)
Implemented fully from scratch using Python, including text preprocessing, n-gram counting, probability computation, and sampling-based text generation.

Task 2 â€” Scaled Dot-Product Attention (Optional)
A NumPy-only implementation of the core operation behind Transformer architectures (BERT, GPT, etc.), including a demo script.

This assignment showcases clean code design, probabilistic modeling, understanding of linear algebra, and modular project structure.

ğŸ“‚ Project Structure
ml-assignment/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ example_corpus.txt
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ngram_model.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â””â”€â”€ generate.py
â”‚
â”œâ”€â”€ attention/
â”‚   â”œâ”€â”€ attention.py
â”‚   â””â”€â”€ demo_attention.py
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_ngram.py
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ evaluation.md
â””â”€â”€ README.md   â† (this file)

ğŸš€ How to Run the Project
1ï¸âƒ£ Create and Activate Virtual Environment
python3 -m venv venv
source venv/bin/activate       # Linux/Mac
venv\Scripts\activate          # Windows

2ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

ğŸŸ¦ TASK 1 â€” TRIGRAM LANGUAGE MODEL
â–¶ï¸ How to Run the Trigram Generator
python -m src.generate


This will:

Read corpus from data/example_corpus.txt

Clean and tokenize text

Train a trigram language model

Print generated text using probabilistic sampling

ğŸ”§ How It Works (Short Explanation)

Text is cleaned â†’ lowercased, punctuation removed

<s> and <e> tokens mark sentence boundaries

Trigrams (w1, w2, w3) are counted in a nested dictionary

Probabilities are computed as:

P(w3 | w1, w2) = count(w1, w2, w3) / sum(count(w1, w2, *))


Text generation starts with <s>, <s> and samples the next words based on these probabilities

Continues until <e> or max_length reached

A full 1-page explanation goes in evaluation.md.

ğŸ§ª Run Pytests
pytest -v


Pytests validate:

model training

text generation

empty text handling

short text behavior

ğŸŸ§ TASK 2 â€” SCALED DOT-PRODUCT ATTENTION (Optional)

This task implements the mathematical attention mechanism from Transformers using only NumPy.

Formula:

Attention(Q, K, V) = softmax( QKáµ€ / âˆšd_k ) Â· V


Where:

Q â†’ Queries

K â†’ Keys

V â†’ Values

d_k â†’ Key dimensionality

â–¶ï¸ How to Run the Attention Demo

Navigate to the attention/ directory:

cd attention


Run:

python demo_attention.py


The script will:

Create random Q, K, V matrices

Call scaled_dot_product_attention()

Print:

Attention Output

Attention Weights (Softmax matrix)

ğŸ§ª Manual Testing (Optional)
import numpy as np
from attention import scaled_dot_product_attention

Q = np.random.rand(1, 3, 4)
K = np.random.rand(1, 3, 4)
V = np.random.rand(1, 3, 4)

output, weights = scaled_dot_product_attention(Q, K, V)
print(output)
print(weights)
